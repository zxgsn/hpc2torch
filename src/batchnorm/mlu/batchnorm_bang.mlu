#include "bang.h"
#include "cnrt.h"
#include "cnnl.h"
#include "cnnl_extra.h"
#include <vector>
const int NRAM_MAX_SIZE = 1024 * 256;
__nram__ char nram_buffer[NRAM_MAX_SIZE];

template<typename T>
__mlu_global__ void batchnormNHWC(T const *input, T const *scale, T const *bias, T const *mean, T const *var, T *output, float eps, int frontsize, int cSize){
    //frontsize = N * H * W, cSize = C
    const int SRC_MAX_SIZE = NRAM_MAX_SIZE / 16;
    const int maxNum = SRC_MAX_SIZE / sizeof(T);
    int taskSize = taskDim * maxNum;
    T *src = (T *)nram_buffer;//[maxNum]
    T *s_src = src + maxNum;
    T *b_src = s_src + maxNum;
    T *m_src = b_src + maxNum;
    T *v_src = m_src + maxNum;
    if(cSize >= taskSize){
        int remain = cSize % taskSize;
        int repeat = (cSize - remain) / taskSize;
        int remainT = remain % taskDim;
        int stepEasy = (remain - remainT) / taskDim;
        int stepHard = stepEasy + 1;
        int step = (taskId < remainT ? stepHard : stepEasy);
        int indStart = repeat * taskSize + (taskId < remainT ? taskId * stepHard : (remainT * stepHard + (taskId - remainT) * stepEasy));
        for(int j = 0; j < frontsize; j++){
            int tid = j * cSize;
            for(int i = 0; i < repeat; i++){
                __memcpy(src, input + tid + i * taskSize + taskId * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                __memcpy(s_src, scale + i * taskSize + taskId * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                __memcpy(b_src, bias + i * taskSize + taskId * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                __memcpy(m_src, mean + i * taskSize + taskId * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                __memcpy(v_src, var + i * taskSize + taskId * maxNum, maxNum * sizeof(T), GDRAM2NRAM);

                __bang_add_scalar(v_src, v_src, static_cast<T>(eps), maxNum);
                __bang_active_rsqrt(v_src, v_src, maxNum);
                __bang_sub(src, src, m_src, maxNum);
                __bang_mul(src, src, s_src, maxNum);
                __bang_mul(src, src, v_src, maxNum);
                __bang_add(src, src, b_src, maxNum);
                __memcpy(output + tid + i * taskSize + taskId * maxNum, src, maxNum * sizeof(T), NRAM2GDRAM);
            }
            if(step){
                __memcpy(src, input + tid + indStart, step * sizeof(T), GDRAM2NRAM);
                __memcpy(s_src, scale + indStart, step * sizeof(T), GDRAM2NRAM);
                __memcpy(b_src, bias + indStart, step * sizeof(T), GDRAM2NRAM);
                __memcpy(m_src, mean + indStart, step * sizeof(T), GDRAM2NRAM);
                __memcpy(v_src, var + indStart, step * sizeof(T), GDRAM2NRAM);

                __bang_add_scalar(v_src, v_src, static_cast<T>(eps), maxNum);
                __bang_active_rsqrt(v_src, v_src, maxNum);
                __bang_sub(src, src, m_src, maxNum);
                __bang_mul(src, src, s_src, maxNum);
                __bang_mul(src, src, v_src, maxNum);
                __bang_add(src, src, b_src, maxNum);
                __memcpy(output + tid + indStart, src, step * sizeof(T), NRAM2GDRAM);
            }
        }
    }
    else if(cSize < taskSize && cSize >= maxNum){
        int remain = frontsize % taskDim;
        int stepEasy = (frontsize - remain) / taskDim;
        int stepHard = stepEasy + 1;
        int step = (taskId < remain ? stepHard : stepEasy);
        int indStart = (taskId < remain ? taskId * stepHard : (remain * stepHard + (taskId - remain) * stepEasy));

        int remainT = cSize % maxNum;
        int repeat = (cSize - remainT) / maxNum;
        for(int j = indStart; j < indStart + step; j++){
            int tid = j * cSize;
            for(int i = 0; i < repeat; i++){
                __memcpy(src, input + tid + i * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                __memcpy(s_src, scale + i * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                __memcpy(b_src, bias + i * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                __memcpy(m_src, mean + i * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                __memcpy(v_src, var + i * maxNum, maxNum * sizeof(T), GDRAM2NRAM);

                __bang_add_scalar(v_src, v_src, static_cast<T>(eps), maxNum);
                __bang_active_rsqrt(v_src, v_src, maxNum);
                __bang_sub(src, src, m_src, maxNum);
                __bang_mul(src, src, s_src, maxNum);
                __bang_mul(src, src, v_src, maxNum);
                __bang_add(src, src, b_src, maxNum);
                __memcpy(output + tid + i * maxNum, src, maxNum * sizeof(T), NRAM2GDRAM);
            }
            if(remainT){
                __memcpy(src, input + tid + repeat * maxNum, remainT * sizeof(T), GDRAM2NRAM);
                __memcpy(s_src, scale + repeat * maxNum, remainT * sizeof(T), GDRAM2NRAM);
                __memcpy(b_src, bias + repeat * maxNum, remainT * sizeof(T), GDRAM2NRAM);
                __memcpy(m_src, mean + repeat * maxNum, remainT * sizeof(T), GDRAM2NRAM);
                __memcpy(v_src, var + repeat * maxNum, remainT * sizeof(T), GDRAM2NRAM);

                __bang_add_scalar(v_src, v_src, static_cast<T>(eps), maxNum);
                __bang_active_rsqrt(v_src, v_src, maxNum);
                __bang_sub(src, src, m_src, maxNum);
                __bang_mul(src, src, s_src, maxNum);
                __bang_mul(src, src, v_src, maxNum);
                __bang_add(src, src, b_src, maxNum);
                __memcpy(output + tid + repeat * maxNum, src, remainT * sizeof(T), NRAM2GDRAM);
            }
        }
    }
    else{
        int multiple = maxNum / cSize;//一个core一次可以处理multiple个cSize
        int taskSize = taskDim * multiple;
        int remainT = frontsize % taskSize;
        int repeat = (frontsize - remainT) / taskSize;
        int remain = remainT % taskDim;
        int stepEasy = (remainT - remain) / taskDim;
        int stepHard = stepEasy + 1;
        int step = (taskId < remain ? stepHard : stepEasy);
        int indStart = (taskId < remain ? taskId * stepHard : (remain * stepHard + (taskId - remain) * stepEasy));
        __memcpy(s_src, scale, cSize * sizeof(T), GDRAM2NRAM);
        __memcpy(b_src, bias, cSize * sizeof(T), GDRAM2NRAM);
        __memcpy(m_src, mean, cSize * sizeof(T), GDRAM2NRAM);
        __memcpy(v_src, var, cSize * sizeof(T), GDRAM2NRAM);
        __bang_add_scalar(v_src, v_src, static_cast<T>(eps), cSize);
        __bang_active_rsqrt(v_src, v_src, cSize);
        int tid;
        for(int i = 0; i < repeat; i++){
            tid = i * taskSize * cSize;
            __memcpy(src, input + tid + taskId * multiple * cSize, multiple * cSize * sizeof(T), GDRAM2NRAM);
            for(int m = 0; m < multiple; m++){
                __bang_sub(src + m * cSize, src + m * cSize, m_src, cSize);
                __bang_mul(src + m * cSize, src + m * cSize, s_src, cSize);
                __bang_mul(src + m * cSize, src + m * cSize, v_src, cSize);
                __bang_add(src + m * cSize, src + m * cSize, b_src, cSize);
            }
            __memcpy(output + tid + taskId * multiple * cSize, src, multiple * cSize * sizeof(T), NRAM2GDRAM);
        }
        if(step){
            tid = (repeat * taskSize + indStart) * cSize;
            __memcpy(src, input + tid, step * cSize * sizeof(T), GDRAM2NRAM);
            for(int m = 0; m < step; m++){
                __bang_sub(src + m * cSize, src + m * cSize, m_src, cSize);
                __bang_mul(src + m * cSize, src + m * cSize, s_src, cSize);
                __bang_mul(src + m * cSize, src + m * cSize, v_src, cSize);
                __bang_add(src + m * cSize, src + m * cSize, b_src, cSize);
            }
            __memcpy(output + tid, src, step * cSize * sizeof(T), NRAM2GDRAM);
        }
    }
}
template<typename T>
void batchnormUnion(cnrtQueue_t queue, void const *input, void const *scale, void const *bias, void const *mean, void const *var, void *output, int *shape, int nDim, float eps){
    int cSize = shape[1];
    int frontsize = 1;
    
    for(int i = 0; i < nDim; i++){
        if(i != 1){
            frontsize *= shape[i];
        }
    }

    cnrtDim3_t k_dim;
    cnrtFunctionType_t k_type;

    k_dim.x = 16;
    k_dim.y = 1;
    k_dim.z = 1;
    
    k_type = CNRT_FUNC_TYPE_UNION1;
    
    auto weight = reinterpret_cast<const T *>(scale);
    auto _bias = reinterpret_cast<const T *>(bias);
    auto _mean = reinterpret_cast<const T *>(mean);
    auto _var = reinterpret_cast<const T *>(var);
    
    if(nDim == 2){
        auto source = reinterpret_cast<const T *>(input);
        auto destination = reinterpret_cast<T *>(output);
        
        batchnormNHWC<T><<<k_dim, k_type, queue>>>(source, weight, _bias, _mean, _var, destination, eps, frontsize, cSize);
        cnrtQueueSync(queue);
    }
    else if(nDim > 2){
        
        std::vector<int> inDim(nDim);//原始input的形状为[n,c,h,w]
        std::vector<int> outDim(nDim);
        int size = 1;
        for (int i = 0; i < nDim; i++) {
            inDim[i] = shape[i];
            outDim[i] = shape[i];
            size *= shape[i];
        }
        cnnlDataType_t dataType;
        if(sizeof(T) == 2){
            dataType = CNNL_DTYPE_HALF;
        }
        else if(sizeof(T) == 4){
            dataType = CNNL_DTYPE_FLOAT;
        }
        cnnlHandle_t handle;
        cnnlCreate(&handle);
        cnnlSetQueue(handle, queue);
        //下面开始针对input做转置，nchw2nhwc
        T *tmpGdramI, *tmpGdramO;//batchnorm库函数只能处理[n,h,w,c],tmpGdramI作为转置来变换input
        CNRT_CHECK(cnrtMalloc((void **)&tmpGdramI, size * sizeof(T)));
        CNRT_CHECK(cnrtMalloc((void **)&tmpGdramO, size * sizeof(T)));
        cnnlTransposeDescriptor_t desc;
        cnnlCreateTransposeDescriptor(&desc);
        
        std::vector<int> permuteI(nDim);
        std::vector<int> permuteO(nDim);
        for (int i = 0; i < nDim; i++) {
            permuteI[i] = i;
            permuteO[i] = i;
        }
        for (int i = 0; i < nDim; i++) {
            if(i >= 1){
                permuteI[i] = i + 1;
            }
            if(i >= 2){
                permuteO[i] = i - 1;
            }
        }
        permuteI[nDim - 1] = 1;
        permuteO[1] = nDim - 1;
        
        cnnlSetTransposeDescriptor(desc, nDim, permuteI.data());

        std::vector<int> tranDim(nDim);//tmpGdramI和tmpGdramO的形状
        for(int i = 0; i < nDim; i++){
            tranDim[i] = shape[permuteI[i]];
        }
        
        cnnlTensorDescriptor_t aDesc, cDesc;
        
        cnnlCreateTensorDescriptor(&aDesc);
        cnnlCreateTensorDescriptor(&cDesc);
        cnnlSetTensorDescriptor(
            aDesc, CNNL_LAYOUT_ARRAY, dataType,
            inDim.size(), inDim.data());
        
        cnnlSetTensorDescriptor(
            cDesc, CNNL_LAYOUT_ARRAY, dataType,
            tranDim.size(), tranDim.data());

        
        size_t tSize;
        cnnlGetTransposeWorkspaceSize(handle, aDesc, desc, &tSize);
        void *workspace;
        cnrtMalloc(&workspace, tSize);
        
        cnnlTranspose_v2(handle, desc, aDesc, input, cDesc,
                                tmpGdramI, workspace, tSize);
        CNRT_CHECK(cnrtQueueSync(queue));          
        //------------------------------------------------------------               
        //上面成功对input做好了nchw2nhwc，下面开始正式计算batchnorm
        batchnormNHWC<T><<<k_dim, k_type, queue>>>(tmpGdramI, weight, _bias, _mean, _var, tmpGdramO, eps, frontsize, cSize);
        CNRT_CHECK(cnrtQueueSync(queue));
        //------------------------------------------------------------ 
        //下面开始提前对output做转置：nhwc2nchw，此时需要重新设置aDesc和cDesc,desc
        cnnlSetTensorDescriptor(
            aDesc, CNNL_LAYOUT_ARRAY, dataType,
            tranDim.size(), tranDim.data());
        cnnlSetTensorDescriptor(
            cDesc, CNNL_LAYOUT_ARRAY, dataType,
            outDim.size(), outDim.data());
        cnnlSetTransposeDescriptor(desc, nDim, permuteO.data());
        cnnlTranspose_v2(handle, desc, aDesc, tmpGdramO, cDesc,
                                output, workspace, tSize);
        cnnlDestroy(handle);
        CNRT_CHECK(cnrtQueueSync(queue));  
        cnrtFree(tmpGdramI);
        cnrtFree(tmpGdramO);
        cnnlDestroyTensorDescriptor(aDesc);
        cnnlDestroyTensorDescriptor(cDesc);
        cnnlDestroyTransposeDescriptor(desc);
    }
}
extern "C" void batchnorm_bang_f32(void const *input, void const *scale, void const *bias, void const *mean, void const *var, void *output, int *shape, int nDim, float eps){
    cnrtQueue_t queue;
    CNRT_CHECK(cnrtSetDevice(0));
    CNRT_CHECK(cnrtQueueCreate(&queue));
    batchnormUnion<float>(queue, input, scale, bias, mean, var, output, shape, nDim, eps);
    CNRT_CHECK(cnrtQueueDestroy(queue));
}
extern "C" void batchnorm_bang_f16(void const *input, void const *scale, void const *bias, void const *mean, void const *var, void *output, int *shape, int nDim, float eps){
    cnrtQueue_t queue;
    CNRT_CHECK(cnrtSetDevice(0));
    CNRT_CHECK(cnrtQueueCreate(&queue));
    batchnormUnion<half>(queue, input, scale, bias, mean, var, output, shape, nDim, eps);
    CNRT_CHECK(cnrtQueueDestroy(queue));
}
